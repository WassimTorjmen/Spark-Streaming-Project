package com.esgi

import org.apache.kafka.clients.producer.{KafkaProducer, ProducerRecord}
import java.util.Properties
import java.net.{URL, HttpURLConnection}
import scala.io.Source
import ujson._

object ProducerKafka {

  val baseUrl =
    "https://datasets-server.huggingface.co/rows?dataset=openfoodfacts%2Fproduct-database&config=default&split=food"

  def main(args: Array[String]): Unit = {

    /* --------- Param√®tres --------- */
    val useAPI      = true          // false ‚áí lit food.parquet
    val jsonPath    = "data/food.parquet" // fichier de teste 
    val batchLength = 100
    val maxOffset   = 3808300          // pour tester apr√®s on fait 3808300 ensuite
    val topic       = "openfood"

    /*Config Kafka */
    val props = new Properties()
    props.put("bootstrap.servers", "localhost:9092")
    props.put("key.serializer",
              "org.apache.kafka.common.serialization.StringSerializer")
              props.put("value.serializer",
              "org.apache.kafka.common.serialization.StringSerializer")
   props.put("max.request.size", "2000000")

    val producer = new KafkaProducer[String, String](props)
    println("Producer Kafka ‚Äì d√©marrage")

  
    if (useAPI) {
      var offset = 0
      while (offset <= maxOffset) {
        val url   = s"$baseUrl&offset=$offset&length=$batchLength"
        val batch = fetchBatchFromAPI(url)          // JSON brut

        if (batch.nonEmpty) {
          producer.send(new ProducerRecord(topic, null, batch))
          println(s"Batch offset=$offset envoy√© (${batch.length} chars)")
          // preview des donn√©es
          val preview = if (batch.length > 200) batch.take(200) + "..." else batch
          println(s"üü¢ Batch offset=$offset envoy√©  (${batch.length} chars)")
          println(s"   ‚Ü≥ Aper√ßu : $preview\n")
          //reduire le temps d'attente
          Thread.sleep(2000)
        } else {
          println(s"API vide √† offset $offset, arr√™t.")
        }
        offset += batchLength
        Thread.sleep(2000)
      }
    } else {
      fetchBatchesFromFile(jsonPath).foreach { batch =>
        producer.send(new ProducerRecord(topic, null, batch))
        println(s"Batch fichier envoy√© (${batch.length} chars)")
        Thread.sleep(1000)
      }
    }

    producer.flush()
    producer.close()
    println(" Fin d‚Äôenvoi ‚Äì producer Kafka ferm√©.")
  }

  /* Helpers*/

  /** T√©l√©charge un bloc JSON (100 lignes) et renvoie la cha√Æne brute. */
  def fetchBatchFromAPI(url: String): String = {
    try {
      val conn = new URL(url).openConnection().asInstanceOf[HttpURLConnection]
      conn.setConnectTimeout(5000)
      conn.setReadTimeout(5000)

      val is   = conn.getInputStream
      val json = Source.fromInputStream(is).mkString
      is.close()
      json
    } catch {
      case e: Exception =>
        println(s" API error : ${e.getMessage}")
        ""
    }
  }

  /** D√©coupe un fichier local en blocs JSON de 100 produits. */
  def fetchBatchesFromFile(path: String): Vector[String] = {
    try {
      val raw   = Source.fromFile(path).mkString
      val root  = ujson.read(raw)
      // √† enlever 
      val rows  = root("rows").arr.map(_("row"))
      rows.grouped(100).map(g => ujson.Arr(g: _*).render()).toVector
    } catch {
      case e: Exception =>
        println(s" File error : ${e.getMessage}")
        Vector.empty
    }
  }
}
