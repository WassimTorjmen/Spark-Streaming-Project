package com.esgi // D√©clare le package dans lequel se trouve ce fichier

import org.apache.kafka.clients.producer.{KafkaProducer, ProducerRecord} // Importe les classes n√©cessaires pour produire des messages Kafka
import java.util.Properties // Pour configurer Kafka
import java.net.{URL, HttpURLConnection} // Pour faire des requ√™tes HTTP
import scala.io.Source // Pour lire les flux d'entr√©e (ex : r√©ponses HTTP)
import ujson._ // Pour parser et manipuler du JSON

object ProducerKafka { // Objet principal contenant le programme

  // URL de base de l'API HuggingFace pour r√©cup√©rer les produits OpenFoodFacts
  val baseUrl = "https://datasets-server.huggingface.co/rows?dataset=openfoodfacts%2Fproduct-database&config=default&split=food"

  def main(args: Array[String]): Unit = { // Point d'entr√©e du programme

    /* --------- Param√®tres --------- */
    //val useAPI      = true 
    //val jsonPath    = "data/food.parquet" 
    val batchLength = 100 // Nombre de produits √† r√©cup√©rer/envoyer par batch
    val maxOffset   = 3808300 // Limite maximale d‚Äôoffset pour la pagination API
    val topic       = "openfood"
    val bootstrap = sys.env.getOrElse("KAFKA_BOOTSTRAP_SERVERS", "kafka:9092") // Adresse du serveur Kafka

    /* --------- Configuration Kafka --------- */
    val props = new Properties() // Cr√©ation d‚Äôun objet de configuration
    props.put("bootstrap.servers", bootstrap) // Serveur Kafka

    // S√©rialisation de la cl√© et de la valeur en String
    props.put("key.serializer", "org.apache.kafka.common.serialization.StringSerializer")
    props.put("value.serializer", "org.apache.kafka.common.serialization.StringSerializer")

    props.put("max.request.size", "2000000") // Taille maximale de la requ√™te (2 Mo)

    val producer = new KafkaProducer[String, String](props) // Cr√©ation du producteur Kafka
    println("Producer Kafka - d√©marrage") 

      var offset = 0 // Offset de d√©part
      while (offset <= maxOffset) { // Boucle jusqu‚Äô√† atteindre la limite d‚Äôoffset
        val url   = s"$baseUrl&offset=$offset&length=$batchLength" // Construit l‚ÄôURL avec les param√®tres
        val batch = fetchBatchFromAPI(url) // R√©cup√®re un batch de produits (JSON brut)

        if (batch.nonEmpty) { // Si le batch n‚Äôest pas vide
          producer.send(new ProducerRecord(topic, null, batch)) // Envoie le batch dans Kafka
          println(s"Batch offset=$offset envoy√© (${batch.length} chars)") // Affiche confirmation + taille du batch

          // Aper√ßu des donn√©es envoy√©es (limit√© √† 200 caract√®res)
          val preview = if (batch.length > 200) batch.take(200) + "..." else batch
          println(s"üü¢ Batch offset=$offset envoy√©  (${batch.length} chars)")
          println(s"   ‚Ü≥ Aper√ßu : $preview\n")

          Thread.sleep(2000) // Pause de 2 secondes pour √©viter de surcharger kafka
        } else {
          println(s"API vide √† offset $offset, arr√™t.") // Si aucune donn√©e re√ßue, on arr√™te
        }

        offset += batchLength // Passe au batch suivant
        Thread.sleep(2000) // Petite pause entre chaque requ√™te pour √©viter de surcharger l'API
      }

    

    producer.flush() // Force l‚Äôenvoi de tous les messages en attente
    producer.close() // Ferme proprement le producteur Kafka
    println(" Fin d'envoi - producer Kafka ferm√©.") // Message de fin
  }

  // Fonction qui r√©cup√®re un batch JSON depuis l‚ÄôAPI
  def fetchBatchFromAPI(url: String): String = {
    try {
      val conn = new URL(url).openConnection().asInstanceOf[HttpURLConnection] // Ouvre une connexion HTTP
      conn.setConnectTimeout(2000)
      conn.setReadTimeout(2000)
      val is   = conn.getInputStream // Ouvre le flux d‚Äôentr√©e de la connexion
      val json = Source.fromInputStream(is).mkString //lire le flux d'entr√©e et le convertir en String
      is.close()
      json
    } catch {
      case e: Exception =>
        println(s"API error : ${e.getMessage}")
        ""
    }
  }

 
}
