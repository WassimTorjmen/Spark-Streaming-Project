# ğŸš€ Spark Streaming Project - Open Food Facts Analytics

Pipeline de traitement de donnÃ©es en temps rÃ©el utilisant **Apache Kafka**, **Apache Spark Structured Streaming**, **PostgreSQL** et **Streamlit** pour analyser les donnÃ©es Open Food Facts.

![Architecture](https://img.shields.io/badge/Architecture-Microservices-blue)
![Scala](https://img.shields.io/badge/Scala-2.12-red)
![Spark](https://img.shields.io/badge/Spark-3.5.5-orange)
![Docker](https://img.shields.io/badge/Docker-Compose-2496ED)

---

## ğŸ“‹ Table des matiÃ¨res

- [Description](#-description)
- [Architecture](#-architecture)
- [Technologies utilisÃ©es](#-technologies-utilisÃ©es)
- [PrÃ©requis](#-prÃ©requis)
- [Installation et dÃ©marrage](#-installation-et-dÃ©marrage)
- [Structure du projet](#-structure-du-projet)
- [Transformations et agrÃ©gations](#-transformations-et-agrÃ©gations)
- [Dashboard Streamlit](#-dashboard-streamlit)
- [Configuration](#-configuration)
- [Auteurs](#-auteurs)

---

## ğŸ“– Description

Ce projet implÃ©mente un **pipeline de streaming de donnÃ©es** complet qui :

1. **Collecte** les donnÃ©es produits depuis l'API Open Food Facts (via HuggingFace Datasets)
2. **Transmet** les donnÃ©es en temps rÃ©el via Apache Kafka
3. **Transforme** et agrÃ¨ge les donnÃ©es avec Spark Structured Streaming
4. **Stocke** les rÃ©sultats dans PostgreSQL
5. **Visualise** les insights via un dashboard Streamlit interactif

---

## ğŸ— Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Open Food     â”‚â”€â”€â”€â”€â–¶â”‚    Kafka    â”‚â”€â”€â”€â”€â–¶â”‚  Spark Consumer â”‚
â”‚   Facts API     â”‚     â”‚   Broker    â”‚     â”‚   (Streaming)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â–²                      â”‚
                              â”‚                      â–¼
                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                     â”‚   Zookeeper     â”‚     â”‚  PostgreSQL  â”‚
                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                                                    â”‚
                                                    â–¼
                                            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                            â”‚  Streamlit   â”‚
                                            â”‚  Dashboard   â”‚
                                            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ›  Technologies utilisÃ©es

| Technologie | Version | RÃ´le |
|-------------|---------|------|
| **Scala** | 2.12.18 | Langage de dÃ©veloppement |
| **Apache Spark** | 3.5.5 | Traitement de donnÃ©es en streaming |
| **Apache Kafka** | 3.5.1 | Message broker |
| **Zookeeper** | 3.9 | Coordination Kafka |
| **PostgreSQL** | 15 | Base de donnÃ©es relationnelle |
| **Streamlit** | latest | Dashboard interactif |
| **Docker Compose** | 3.9 | Containerisation et orchestration |

---

## âœ… PrÃ©requis

- **Docker** et **Docker Compose** installÃ©s sur votre machine
- **8 Go de RAM** minimum recommandÃ©s
- **Ports disponibles** : 2181 (Zookeeper), 9092 (Kafka), 5433 (PostgreSQL), 8501 (Streamlit)

---

## ğŸš€ Installation et dÃ©marrage

### 1. Cloner le repository

```bash
git clone https://github.com/WassimTorjmen/Spark-Streaming-Project.git
cd Spark-Streaming-Project
```

### 2. Lancer l'infrastructure

```bash
docker compose up --build -d
```

### 3. VÃ©rifier que tous les services sont actifs

```bash
docker compose ps
```

### 4. AccÃ©der au dashboard

Ouvrez votre navigateur Ã  l'adresse : **http://localhost:8501**

### 5. ArrÃªter les services

```bash
docker compose down
```

Pour supprimer Ã©galement les volumes (donnÃ©es persistantes) :

```bash
docker compose down -v
```

---

## ğŸ“ Structure du projet

```
Spark-Streaming-Project/
â”œâ”€â”€ docker-compose.yml          # Orchestration des services
â”œâ”€â”€ pom.xml                     # Configuration Maven (dÃ©pendances Scala/Spark)
â”œâ”€â”€ Dockerfile                  # Image Docker principale
â”‚
â”œâ”€â”€ src/main/scala/com/esgi/
â”‚   â”œâ”€â”€ Main.scala              # Point d'entrÃ©e principal
â”‚   â”œâ”€â”€ Producer/
â”‚   â”‚   â”œâ”€â”€ Producer.scala      # Producteur Kafka (rÃ©cupÃ¨re les donnÃ©es API)
â”‚   â”‚   â””â”€â”€ Dockerfile
â”‚   â””â”€â”€ Consumer/
â”‚       â”œâ”€â”€ Consumer.scala      # Consommateur Spark Streaming
â”‚       â””â”€â”€ Dockerfile
â”‚
â”œâ”€â”€ Infra/
â”‚   â”œâ”€â”€ kafka/
â”‚   â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”‚   â””â”€â”€ server.properties
â”‚   â”œâ”€â”€ postgres/
â”‚   â”‚   â””â”€â”€ init.sql            # Script d'initialisation des tables
â”‚   â””â”€â”€ docker-compose.yml
â”‚
â”œâ”€â”€ streamlit/
â”‚   â”œâ”€â”€ app.py                  # Application Streamlit
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ requirements.txt
â”‚
â””â”€â”€ data/                       # DonnÃ©es locales (optionnel)
```

---

## ğŸ”„ Transformations et agrÃ©gations

Le consumer Spark effectue les transformations suivantes :

| Table | Description |
|-------|-------------|
| `nutriscore_counts` | RÃ©partition des produits par Nutriscore (A, B, C, D, E) |
| `category_counts` | Nombre de produits par catÃ©gorie principale |
| `brand_counts` | Nombre de produits par marque |
| `packaging_distribution` | RÃ©partition par type d'emballage |
| `top_additive_products` | Top 10 des produits avec le plus d'additifs |
| `nova_group_classification` | Classification NOVA (niveau de transformation) |
| `top_sugary_products_by_category` | Produits les plus sucrÃ©s par catÃ©gorie |

---

## ğŸ“Š Dashboard Streamlit

Le dashboard offre les visualisations suivantes :

- **ğŸ“ˆ Graphique en barres** : RÃ©partition des produits par Nutriscore
- **ğŸ¥§ Diagramme circulaire** : Top 9 des catÃ©gories principales
- **ğŸ“Š Graphique en barres** : Top 10 des marques
- **ğŸ”µ Donut chart** : Types d'emballage les plus courants
- **ğŸ“‹ Tableau** : Produits avec le plus d'additifs
- **ğŸ“‰ Graphique NOVA** : Classification par niveau de transformation

---

## âš™ï¸ Configuration

### Variables d'environnement

| Variable | Valeur par dÃ©faut | Description |
|----------|-------------------|-------------|
| `KAFKA_BOOTSTRAP_SERVERS` | `kafka:9092` | Adresse du broker Kafka |
| `BATCH_LENGTH` | `100` | Nombre de produits par batch |
| `MAX_OFFSET` | `3808300` | Offset maximum pour l'API |
| `PG_URL` | `jdbc:postgresql://postgres:5432/openfood` | URL PostgreSQL |
| `PG_USER` | `ingest` | Utilisateur PostgreSQL |
| `PG_PWD` | `ingestpwd` | Mot de passe PostgreSQL |
| `CHECKPOINT_PATH` | `/checkpoint/generic` | Chemin des checkpoints Spark |

### Ports exposÃ©s

| Service | Port |
|---------|------|
| Zookeeper | 2181 |
| Kafka | 9092 |
| PostgreSQL | 5433 (mappÃ© vers 5432 interne) |
| Streamlit | 8501 |

---

## ğŸ”§ Commandes utiles

```bash
# Voir les logs d'un service spÃ©cifique
docker compose logs -f producer
docker compose logs -f consumer
docker compose logs -f streamlit

# AccÃ©der Ã  PostgreSQL
docker exec -it postgres psql -U ingest -d openfood

# VÃ©rifier les tables (dans psql)
# \dt

# Voir les donnÃ©es Nutriscore (dans psql)

# SELECT * FROM nutriscore_counts;

# Reconstruire un service spÃ©cifique
docker compose up --build -d consumer
```

---

## ğŸ› DÃ©pannage

| ProblÃ¨me | Solution |
|----------|----------|
| Kafka ne dÃ©marre pas | VÃ©rifiez que Zookeeper est actif : `docker compose logs zookeeper` |
| Consumer n'Ã©crit pas dans PostgreSQL | VÃ©rifiez les credentials et que la base `openfood` existe |
| Streamlit affiche "Connexion Ã©chouÃ©e" | Attendez que PostgreSQL soit initialisÃ© (30-60 secondes) |
| MÃ©moire insuffisante | Augmentez la RAM Docker ou rÃ©duisez `BATCH_LENGTH` |

---

## ğŸ‘¥ Auteurs

- **Wassim Torjmen** - *DÃ©veloppeur principal*

---

## ğŸ“„ Licence

Ce projet est sous licence MIT.

---

## ğŸ™ Remerciements

- [Open Food Facts](https://world.openfoodfacts.org/) pour les donnÃ©es alimentaires ouvertes
- [HuggingFace Datasets](https://huggingface.co/datasets/openfoodfacts/product-database) pour l'API de donnÃ©es
